{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58417df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              snippet  \\\n",
      "0   Jul 31, 2023 — ... food & beverages, staff ser...   \n",
      "1   Aug 3, 2022 — has opened a new outlet at Depar...   \n",
      "2   Sep 25, 2023 — Bangalore airport also earns 70...   \n",
      "3   Dec 15, 2020 — The Quad retail and food and be...   \n",
      "4   BLR Airport. @BLRAirport. The official account...   \n",
      "..                                                ...   \n",
      "85  Let us join together to make Bangalore City a ...   \n",
      "86  But in Bangalore, you also need to look sidewa...   \n",
      "87  Day 5 of #IsraelPalestineWar | Israel retakes ...   \n",
      "88  ... food to the people to support them physica...   \n",
      "89  Aug 31, 2023 — Cheers from the International L...   \n",
      "\n",
      "                                            highlighs sentiment  \n",
      "0   ['food', 'beverages', 'Kempegowda Internationa...   Neutral  \n",
      "1                                            ['food']  Positive  \n",
      "2         ['Bangalore airport', 'Food and beverages']   Neutral  \n",
      "3   ['food and beverage', 'Kempegowda Internationa...   Neutral  \n",
      "4   ['BLR Airport', 'Kempegowda International Airp...   Neutral  \n",
      "..                                                ...       ...  \n",
      "85                         ['Bangalore', 'Bengaluru']  Positive  \n",
      "86               ['Bangalore', 'food', 'restaurants']   Neutral  \n",
      "87                                                NaN  Negative  \n",
      "88                                           ['food']  Negative  \n",
      "89   ['International', 'Bangalore airport', 'flight']  Positive  \n",
      "\n",
      "[90 rows x 3 columns]\n",
      "Number of Positive Sentiments: 53\n",
      "Number of Negative Sentiments: 6\n",
      "Number of Neutral Sentiments: 31\n",
      "postitive_response 58.88888888888889\n",
      "negative_response 6.666666666666667\n",
      "neutral_response 34.44444444444444\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data = pd.read_excel(\"D:\\\\Internship Progress\\\\Final all 3 datasets.xlsx\")\n",
    "\n",
    "text_column = 'snippet'\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['sentiment'] = data[text_column].apply(classify_sentiment)\n",
    "\n",
    "data.to_excel(\"D:\\Internship Progress\\Final all 3 datasetSENTIMENTS.xlsx\", index=False)\n",
    "\n",
    "print(data)\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "count_neutral = 0\n",
    "\n",
    "for sentiment in data['sentiment']:\n",
    "    if sentiment == 'Positive':\n",
    "        count_positive += 1\n",
    "    elif sentiment == 'Negative':\n",
    "        count_negative += 1\n",
    "    elif sentiment == 'Neutral':\n",
    "        count_neutral += 1\n",
    "\n",
    "print(f\"Number of Positive Sentiments: {count_positive}\")\n",
    "print(f\"Number of Negative Sentiments: {count_negative}\")\n",
    "print(f\"Number of Neutral Sentiments: {count_neutral}\")\n",
    "\n",
    "per1 = 0\n",
    "per2 = 0\n",
    "per3 = 0\n",
    "total = len(data)\n",
    "postitive_response = (count_positive / total) * 100\n",
    "negative_response = (count_negative / total) * 100\n",
    "neutral_response = (count_neutral / total) * 100\n",
    "\n",
    "print(\"postitive_response\", postitive_response)\n",
    "print(\"negative_response\", negative_response)\n",
    "print(\"neutral_response\", neutral_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c87d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              snippet  \\\n",
      "0   Bravo Mumbai Immigration &Customs! They've rem...   \n",
      "1   - Application form - Cover letter with itinera...   \n",
      "2   3AM immigration queues at Bangalore Airport. I...   \n",
      "3   Myy son studying in the US landed at Delhi air...   \n",
      "4   Bangalore airport experience was stellar - tru...   \n",
      "..                                                ...   \n",
      "95  ... International Airport). More than that, it...   \n",
      "96  Parsis are Irani Migrants who landed & settled...   \n",
      "97  I booked my flight for Bengaluru but I asked t...   \n",
      "98  Jan 10, 2020 — Dear Mohit, We would like to in...   \n",
      "99  I'm seeking your help regarding my father's pa...   \n",
      "\n",
      "                                            highlighs sentiment  \n",
      "0                 ['Immigration', 'forms', 'airport']  Negative  \n",
      "1   ['Application form', 'flight', 'entry', 'airpo...  Negative  \n",
      "2        ['immigration', 'Bangalore Airport', 'form']   Neutral  \n",
      "3             ['airport', 'Bangalore', 'immigration']  Negative  \n",
      "4                ['Bangalore airport', 'immigration']  Negative  \n",
      "..                                                ...       ...  \n",
      "95  ['International Airport', 'arrivals', 'passport']  Negative  \n",
      "96                                       ['Migrants']   Neutral  \n",
      "97                                     ['no problem']  Positive  \n",
      "98  ['you can carry upto 2 litres of alcohol purch...  Positive  \n",
      "99   ['passport', 'airport immigration', 'departure']  Positive  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Number of Positive Sentiments: 32\n",
      "Number of Negative Sentiments: 47\n",
      "Number of Neutral Sentiments: 21\n",
      "postitive_response 32.0\n",
      "negative_response 47.0\n",
      "neutral_response 21.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data = pd.read_excel(\"D:\\Internship Progress\\immigration_cleaned.xlsx\")\n",
    "text_column = 'snippet'\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['sentiment'] = data[text_column].apply(classify_sentiment)\n",
    "\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Final all 3 datasetSENTIMENTS2.xlsx\", index=False)\n",
    "\n",
    "print(data)\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "count_neutral = 0\n",
    "\n",
    "for sentiment in data['sentiment']:\n",
    "    if sentiment == 'Positive':\n",
    "        count_positive += 1\n",
    "    elif sentiment == 'Negative':\n",
    "        count_negative += 1\n",
    "    elif sentiment == 'Neutral':\n",
    "        count_neutral += 1\n",
    "\n",
    "print(f\"Number of Positive Sentiments: {count_positive}\")\n",
    "print(f\"Number of Negative Sentiments: {count_negative}\")\n",
    "print(f\"Number of Neutral Sentiments: {count_neutral}\")\n",
    "\n",
    "per1 = 0\n",
    "per2 = 0\n",
    "per3 = 0\n",
    "total = len(data)\n",
    "postitive_response = (count_positive / total) * 100\n",
    "negative_response = (count_negative / total) * 100\n",
    "neutral_response = (count_neutral / total) * 100\n",
    "\n",
    "print(\"postitive_response\", postitive_response)\n",
    "print(\"negative_response\", negative_response)\n",
    "print(\"neutral_response\", neutral_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29516e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              snippet  \\\n",
      "0   May 22, 2019 — Dive into the luxury and unmatc...   \n",
      "1   Taj Bangalore is a mere walk away from the Kem...   \n",
      "2   Sep 14, 2018 — We're offering a full range of ...   \n",
      "3   Red Key is a budget-friendly 2-star hotel in B...   \n",
      "4   Sep 14, 2018 — We're offering a full range of ...   \n",
      "..                                                ...   \n",
      "94  for making this tour happen The security was t...   \n",
      "95  Thank you for your wonderful hospitality and s...   \n",
      "96  Free event invite on Amex Platinum card at ITC...   \n",
      "97  YCO contestants getting ready for cook off at ...   \n",
      "98  for your wonderful hospitality in Germany.    ...   \n",
      "\n",
      "                                            highlighs sentiment  \n",
      "0   ['hospitality', 'Bangalore', 'Kempegowda Inter...  Negative  \n",
      "1   ['Bangalore', 'International Airport', 'hotel'...  Negative  \n",
      "2                          ['Hospitality', 'airport']   Neutral  \n",
      "3    ['hotel in Bangalore', 'Airport', 'hospitality']  Positive  \n",
      "4                          ['Hospitality', 'airport']   Neutral  \n",
      "..                                                ...       ...  \n",
      "94                                    ['hospitality']  Positive  \n",
      "95               ['hospitality', 'Bengaluru airport']  Positive  \n",
      "96                 ['hotels in Bengaluru', 'airport']  Positive  \n",
      "97                           ['Bangalore', 'airport']  Positive  \n",
      "98                       ['hospitality', 'Bangalore']  Positive  \n",
      "\n",
      "[99 rows x 3 columns]\n",
      "Number of Positive Sentiments: 58\n",
      "Number of Negative Sentiments: 14\n",
      "Number of Neutral Sentiments: 27\n",
      "postitive_response 58.58585858585859\n",
      "negative_response 14.14141414141414\n",
      "neutral_response 27.27272727272727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data = pd.read_excel(\"D:\\Internship Progress\\Hospitality_cleaned.xlsx\")\n",
    "text_column = 'snippet'\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['sentiment'] = data[text_column].apply(classify_sentiment)\n",
    "\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Final all 3 datasetSENTIMENTS3.xlsx\", index=False)\n",
    "\n",
    "print(data)\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "count_neutral = 0\n",
    "\n",
    "for sentiment in data['sentiment']:\n",
    "    if sentiment == 'Positive':\n",
    "        count_positive += 1\n",
    "    elif sentiment == 'Negative':\n",
    "        count_negative += 1\n",
    "    elif sentiment == 'Neutral':\n",
    "        count_neutral += 1\n",
    "\n",
    "print(f\"Number of Positive Sentiments: {count_positive}\")\n",
    "print(f\"Number of Negative Sentiments: {count_negative}\")\n",
    "print(f\"Number of Neutral Sentiments: {count_neutral}\")\n",
    "\n",
    "per1 = 0\n",
    "per2 = 0\n",
    "per3 = 0\n",
    "total = len(data)\n",
    "postitive_response = (count_positive / total) * 100\n",
    "negative_response = (count_negative / total) * 100\n",
    "neutral_response = (count_neutral / total) * 100\n",
    "\n",
    "print(\"postitive_response\", postitive_response)\n",
    "print(\"negative_response\", negative_response)\n",
    "print(\"neutral_response\", neutral_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ebbedc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airport: 5.89533371218242\n",
      "bangalore: 5.394136358716413\n",
      "food: 4.823487634552764\n",
      "bengaluru: 2.7701025085776556\n",
      "international: 2.630780233823315\n",
      "india: 2.1741076509102384\n",
      "2023: 2.053593533435689\n",
      "image: 1.8536207437200516\n",
      "beverages: 1.728407148835584\n",
      "blr: 1.6604784776489478\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "a = pd.read_excel(\"D:\\Internship Progress\\Food and Beverages cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name].tolist()\n",
    "\n",
    "tokenized_data = [word_tokenize(text) for text in corpus]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_data = [[word.lower() for word in text if word.isalnum() and word.lower() not in stop_words] for text in tokenized_data]\n",
    "\n",
    "preprocessed_data = [' '.join(text) for text in filtered_data]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "word_tfidf_pairs = [(word, score) for word, score in zip(feature_names, tfidf_scores)]\n",
    "\n",
    "word_tfidf_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_keywords = word_tfidf_pairs[:10]\n",
    "for keyword, score in top_keywords:\n",
    "    print(f\"{keyword}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fc85fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\91831\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\91831\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\91831\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc440101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d06fa7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee21c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-rake in c:\\users\\91831\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-rake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcc98b69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "a = pd.read_excel(\"D:\\\\Internship Progress\\\\Food and Beverages cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    keywords = [word for word, freq in freq_dist.most_common(5)]\n",
    "    return keywords\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    keywords = extract_keywords(text)\n",
    "    data['Keywords'] = data[column_name].apply(extract_keywords)\n",
    "data\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Food and Beverages cleaned.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef5fb0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "a = pd.read_excel(\"D:\\\\Internship Progress\\\\Hospitality_cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    keywords = [word for word, freq in freq_dist.most_common(5)]\n",
    "    return keywords\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    keywords = extract_keywords(text)\n",
    "    data['Keywords'] = data[column_name].apply(extract_keywords)\n",
    "data\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\Hospitality_cleaned.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ca85c11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "a = pd.read_excel(\"D:\\\\Internship Progress\\\\immigration_cleaned.xlsx\")\n",
    "data = pd.DataFrame(a)\n",
    "\n",
    "column_name = 'snippet'\n",
    "corpus = data[column_name]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    freq_dist = FreqDist(words)\n",
    "    keywords = [word for word, freq in freq_dist.most_common(5)]\n",
    "    return keywords\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    keywords = extract_keywords(text)\n",
    "    data['Keywords'] = data[column_name].apply(extract_keywords)\n",
    "data\n",
    "data.to_excel(\"D:\\\\Internship Progress\\\\immigration_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422343b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
