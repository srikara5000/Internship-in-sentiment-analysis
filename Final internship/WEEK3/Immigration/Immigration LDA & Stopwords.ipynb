{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fbd2f8",
   "metadata": {},
   "source": [
    "# Immigration-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c44ba9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            full_text sentiment\n",
      "0   immigration international arrivals bangalore a...  Positive\n",
      "1   sir nothing beats efficiency bangalore airport...  Negative\n",
      "2   immigration process bangalore airport far bett...  Positive\n",
      "3   recently interesting conversation immigration ...  Positive\n",
      "4   bangalore airport arrival lounge amazingly goo...  Positive\n",
      "..                                                ...       ...\n",
      "94  woman 2 small kids stuck 4 days bangalore airp...  Positive\n",
      "95  Today Bengaluru Airport immigration section as...   Neutral\n",
      "96  hello left bag airport express coach 26th July...  Positive\n",
      "97  Bangalore 2008 piece immigration form hand cop...  Negative\n",
      "98  Bangalore airport experience stellar truly con...  Negative\n",
      "\n",
      "[99 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download the stopwords if you haven't already\n",
    "\n",
    "# Define a function to remove stopwords from a tweet\n",
    "def remove_stopwords(tweet):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\immigration_1.csv\"\n",
    "output_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\immigration_1.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tweet' column\n",
    "df['full_text'] = df['full_text'].apply(remove_stopwords)\n",
    "print(df)\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d79f34",
   "metadata": {},
   "source": [
    "# Passport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471dc43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            full_text sentiment\n",
      "0   stopped leaving india bangalore airport exit c...  Negative\n",
      "1   sos wife chinese passport valid oci card holde...  Positive\n",
      "2   mam indian passport valid 5 months still immig...  Positive\n",
      "3   experience earlier month bangalore airport des...  Negative\n",
      "4   desi british passport holder always communicat...   Neutral\n",
      "..                                                ...       ...\n",
      "70  traveller bangalore airport led dark room aske...   Neutral\n",
      "71  immigration officer bangalore airport refused ...  Negative\n",
      "72  stopped leaving india bangalore airport exit c...  Negative\n",
      "73  bangalore airport traveller asked fold money p...  Positive\n",
      "74  thank inspector ashish mr kumndan effort searc...  Positive\n",
      "\n",
      "[75 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download the stopwords if you haven't already\n",
    "\n",
    "# Define a function to remove stopwords from a tweet\n",
    "def remove_stopwords(tweet):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\passport.csv\"\n",
    "output_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\passport.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tweet' column\n",
    "df['full_text'] = df['full_text'].apply(remove_stopwords)\n",
    "print(df)\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf59e9c",
   "metadata": {},
   "source": [
    "# Security check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fca82ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            full_text sentiment\n",
      "0          bangalore airport security check organized  Positive\n",
      "1   bangalore airport security check process one s...  Positive\n",
      "2   finally help friends booked qatar airways flig...  Positive\n",
      "3   beside bangalore airport security check 29th m...  Positive\n",
      "4   bangalore airport security check today 3 queue...   Neutral\n",
      "5   massive traffic jam since 4 30 pm full queues ...  Positive\n",
      "6   normally since trips short take shoulder bag c...  Positive\n",
      "7   carry id flying bangalore chennai amp back che...  Positive\n",
      "8              crowd bangalore airport security check  Positive\n",
      "9   update state wise regulations uttarakhand asks...   Neutral\n",
      "10  sir men remove every single item bags emptying...   Neutral\n",
      "11  today bangalore airport security check hand ba...  Positive\n",
      "12  comedy airport security check sharing asked le...  Positive\n",
      "13  expect delays bangalore airport security check...  Positive\n",
      "14  bangalore airport security check pathetically ...  Negative\n",
      "15  bangalore airport security check gone crazier ...  Positive\n",
      "16  today bangalore airport security extremely ent...  Negative\n",
      "17  70 mins highway practically takes 2 5 hours on...  Positive\n",
      "18  traveling trivandrum bangalore onwards mumbai ...  Positive\n",
      "19  nenu hyd mumbai via bangalore flight chesaaru ...  Positive\n",
      "20  surprised see collection deodorant bangalore a...  Positive\n",
      "21  forget wrist watch bangalore airport security ...  Positive\n",
      "22  bangalore airport security checking slow takes...   Neutral\n",
      "23  haha mom gave pure desi ghee main ingredient h...  Positive\n",
      "24  new parents son born bangalore returning bomba...  Positive\n",
      "25  knife went undetectd security check hand lugga...  Positive\n",
      "26  lmao tell story la took bag bangalore airport ...  Positive\n",
      "27  bangalore airport security check started askin...  Positive\n",
      "28  bangalore airport security threw away water bo...  Positive\n",
      "29  allow board chennai bangalore 11 10pm flight r...  Positive\n",
      "30  travelling bangalore international airport nag...  Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download the stopwords if you haven't already\n",
    "\n",
    "# Define a function to remove stopwords from a tweet\n",
    "def remove_stopwords(tweet):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\security check.csv\"\n",
    "output_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\security check.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tweet' column\n",
    "df['full_text'] = df['full_text'].apply(remove_stopwords)\n",
    "print(df)\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51273dec",
   "metadata": {},
   "source": [
    "# Visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20aba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            full_text sentiment\n",
      "0   recently scammed airasia bangalore airport ind...  Negative\n",
      "1   kushanmitra canada leverage 5 eyes country lot...   Neutral\n",
      "2   coral card visa platinum international card ac...  Negative\n",
      "3   many many passengers facing inconvenience etih...  Positive\n",
      "4   good experience flights bad experience airvist...  Negative\n",
      "5   paytm visa debit card accessible bangalore air...   Neutral\n",
      "6   thrishul travelling choice difficult days visa...  Positive\n",
      "7   able access lounge bangalore airport using hsb...  Positive\n",
      "8   bizarre rule lots issues visas flights etc ban...  Negative\n",
      "9   worst pathetic service bangalore airport allow...  Negative\n",
      "10  sorry figure mumbai bangalore airport lounges ...  Negative\n",
      "11  bangalore airport staff needs educated schenge...  Positive\n",
      "12  went checkin security visa check 35 minutes ba...  Negative\n",
      "13  stuck bangalore airport visa issue anyway reac...  Positive\n",
      "14  yeah cousin denied free entry visa card though...  Positive\n",
      "15  waiting bangalore airport domestic longue staf...   Neutral\n",
      "16  well ba staff bangalore airport following ukgo...  Negative\n",
      "17  respected honourable minister iam travelling 2...  Positive\n",
      "18  travelling e visa dubai 22 october 0130 pm ban...   Neutral\n",
      "19  need clarification indians valid us visa eligi...  Positive\n",
      "20  sir urgent please guide us problem bangalore a...  Negative\n",
      "21  student stranded bangalore holding new family ...  Positive\n",
      "22  dear sir kindly help us lost job visa canceled...  Positive\n",
      "23  dear embassy india send wife back india 7 mont...  Positive\n",
      "24  omanair urgent help needed passenger bangalore...  Positive\n",
      "25  sister h4 visa holder denied board flight bang...  Negative\n",
      "26  stranded bangalore airport refused entry india...  Negative\n",
      "27  bangalore airport lounge late serving one wors...  Negative\n",
      "28  bangalore airport lounge denied acess visa car...  Negative\n",
      "29  5 n lesson learnt cheapest get cash forex atm ...   Neutral\n",
      "30  living bangalore understand visa entry bangalo...   Neutral\n",
      "31  yes got big fight bangalore airport hindi offi...  Positive\n",
      "32  immigration authorities bangalore airport surp...  Positive\n",
      "33  respected sushmaswaraj meaindia one guest stuc...  Positive\n",
      "34  lounge access bangalore airport limited master...  Negative\n",
      "35  customs officer bangalore airport know bangkok...  Positive\n",
      "36  visa card denied plaza lounge bangalore airpor...  Negative\n",
      "37  kirenrijiju honble minister dragonfly organic ...  Positive\n",
      "38  denied entry bangalore airport hv e tourist vi...  Negative\n",
      "39  ur credit cards visa allowed access bangalore ...  Positive\n",
      "40  ur ground staff bangalore airport know rules p...  Positive\n",
      "41  bangalore airport stands third e tourist visa ...   Neutral\n",
      "42  australian man refused entry india despite val...  Negative\n",
      "43  guest mine japanese citizen stuck bangalore ai...  Negative\n",
      "44  bangalore airport took five seconds turn 15 ye...  Positive\n",
      "45  bangalore airport immigration baggage delivery...  Positive\n",
      "46  helpsingapore immigration website parents stuc...  Negative\n",
      "47  bangalore airport starts 30 day visa arrival j...   Neutral\n",
      "48  indian visa arrival bangalore airport foreign ...   Neutral\n",
      "49  karnataka hc acquits woman officer bribery cha...  Negative\n",
      "50  bangalore visa arrival foreigners bangalore ai...   Neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91831\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download the stopwords if you haven't already\n",
    "\n",
    "# Define a function to remove stopwords from a tweet\n",
    "def remove_stopwords(tweet):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Read the CSV file\n",
    "input_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\visa.csv\"\n",
    "output_csv_file =\"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\visa.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tweet' column\n",
    "df['full_text'] = df['full_text'].apply(remove_stopwords)\n",
    "print(df)\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd20c3",
   "metadata": {},
   "source": [
    "# Immigration-1 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1519c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Load the input CSV file\n",
    "input_csv_file =\"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\immigration_1.csv\"\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Tokenize the tweets and create a dictionary and corpus\n",
    "texts = [tweet.split() for tweet in df['full_text']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5  # You can adjust this number\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Calculate word frequency by topic\n",
    "word_frequency_by_topic = {topic_id: Counter() for topic_id in range(num_topics)}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    tweet_bow = dictionary.doc2bow(tweet.split())\n",
    "    topics = lda_model[tweet_bow]\n",
    "    for topic, probability in topics:\n",
    "        word_frequency_by_topic[topic].update(tweet.split())\n",
    "\n",
    "# Store word frequencies in a new CSV file\n",
    "output_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\immigration_1LDA.csv\"\n",
    "\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Topic', 'Word', 'Frequency'])\n",
    "\n",
    "    for topic_id, word_frequency in word_frequency_by_topic.items():\n",
    "        for word, frequency in word_frequency.items():\n",
    "            csv_writer.writerow([topic_id, word, frequency])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a04f1d",
   "metadata": {},
   "source": [
    "# PassportLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23c0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Load the input CSV file\n",
    "input_csv_file =\"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\passport.csv\"\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Tokenize the tweets and create a dictionary and corpus\n",
    "texts = [tweet.split() for tweet in df['full_text']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5  # You can adjust this number\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Calculate word frequency by topic\n",
    "word_frequency_by_topic = {topic_id: Counter() for topic_id in range(num_topics)}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    tweet_bow = dictionary.doc2bow(tweet.split())\n",
    "    topics = lda_model[tweet_bow]\n",
    "    for topic, probability in topics:\n",
    "        word_frequency_by_topic[topic].update(tweet.split())\n",
    "\n",
    "# Store word frequencies in a new CSV file\n",
    "output_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\passportLDA.csv\"\n",
    "\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Topic', 'Word', 'Frequency'])\n",
    "\n",
    "    for topic_id, word_frequency in word_frequency_by_topic.items():\n",
    "        for word, frequency in word_frequency.items():\n",
    "            csv_writer.writerow([topic_id, word, frequency])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9327218",
   "metadata": {},
   "source": [
    "# Security CheckLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a43ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Load the input CSV file\n",
    "input_csv_file =\"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\security check.csv\"\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Tokenize the tweets and create a dictionary and corpus\n",
    "texts = [tweet.split() for tweet in df['full_text']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5  # You can adjust this number\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Calculate word frequency by topic\n",
    "word_frequency_by_topic = {topic_id: Counter() for topic_id in range(num_topics)}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    tweet_bow = dictionary.doc2bow(tweet.split())\n",
    "    topics = lda_model[tweet_bow]\n",
    "    for topic, probability in topics:\n",
    "        word_frequency_by_topic[topic].update(tweet.split())\n",
    "\n",
    "# Store word frequencies in a new CSV file\n",
    "output_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\security checkLDA.csv\"\n",
    "\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Topic', 'Word', 'Frequency'])\n",
    "\n",
    "    for topic_id, word_frequency in word_frequency_by_topic.items():\n",
    "        for word, frequency in word_frequency.items():\n",
    "            csv_writer.writerow([topic_id, word, frequency])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ea3e1",
   "metadata": {},
   "source": [
    "# VisaLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607ec783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Load the input CSV file\n",
    "input_csv_file =\"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\visa.csv\"\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Tokenize the tweets and create a dictionary and corpus\n",
    "texts = [tweet.split() for tweet in df['full_text']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5  # You can adjust this number\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Calculate word frequency by topic\n",
    "word_frequency_by_topic = {topic_id: Counter() for topic_id in range(num_topics)}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    tweet_bow = dictionary.doc2bow(tweet.split())\n",
    "    topics = lda_model[tweet_bow]\n",
    "    for topic, probability in topics:\n",
    "        word_frequency_by_topic[topic].update(tweet.split())\n",
    "\n",
    "# Store word frequencies in a new CSV file\n",
    "output_csv_file = \"D:\\\\Internship Progress\\\\WEEK3\\\\Immigration\\\\visaLDA.csv\"\n",
    "\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Topic', 'Word', 'Frequency'])\n",
    "\n",
    "    for topic_id, word_frequency in word_frequency_by_topic.items():\n",
    "        for word, frequency in word_frequency.items():\n",
    "            csv_writer.writerow([topic_id, word, frequency])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e51a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
